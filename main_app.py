import streamlit as st
import os
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_chroma import Chroma
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from dotenv import load_dotenv

# ---------------------------------------------------------
# 1. AYARLAR VE YAPILANDIRMA
# ---------------------------------------------------------
st.set_page_config(page_title="RAG AsistanÄ±", page_icon="ğŸ¤–")
st.title("ğŸ“„ PDF Sohbet AsistanÄ±")
st.markdown("*Ders notlarÄ±nÄ±zla konuÅŸun...*")

load_dotenv()

PDF_YOLU = "data/NLP13.pdf"
DB_YOLU = "./chroma_db_deposu2"


# ---------------------------------------------------------
# 2. VERÄ°TABANI OLUÅTURMA (CACHING Ä°LE)
# ---------------------------------------------------------
@st.cache_resource
def get_retriever():
    with st.spinner("Yapay zeka modelleri ve veritabanÄ± yÃ¼kleniyor... (Ä°lk aÃ§Ä±lÄ±ÅŸ 1-2 dk sÃ¼rebilir)"):
        if not os.path.exists(DB_YOLU):
            loader = PyPDFLoader(PDF_YOLU)
            belge = loader.load()
            text_splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=150, length_function=len)
            parcalar = text_splitter.split_documents(belge)
            embedding_model = HuggingFaceEmbeddings(model_name="all-mpnet-base-v2")
            vector_db = Chroma.from_documents(documents=parcalar, embedding=embedding_model, persist_directory=DB_YOLU)
        else:
            embedding_model = HuggingFaceEmbeddings(model_name="all-mpnet-base-v2")
            vector_db = Chroma(persist_directory=DB_YOLU, embedding_function=embedding_model)
        # k=6 diyerek okuma kapasitesini artÄ±rÄ±yoruz (Eskiden 4'tÃ¼)
            # k=10 yaparak daha geniÅŸ bir alanÄ± taramasÄ±nÄ± saÄŸlÄ±yoruz
            return vector_db.as_retriever(search_kwargs={"k": 10})


# ---------------------------------------------------------
# 3. YAPAY ZEKA ZÄ°NCÄ°RÄ°NÄ° (CHAIN) KURMA
# ---------------------------------------------------------
def get_chain(retriever):
    # âœ… DÃœZELTÄ°LEN KISIM: ChatGoogleGenerativeAI ismi doÄŸru
    llm = ChatGoogleGenerativeAI(model="gemini-2.5-flash", temperature=0.3)

    # EKSÄ°K OLAN KISIMLAR BURADAN Ä°TÄ°BAREN BAÅLIYOR:
    template = """
    Sen yardÄ±msever bir asistansÄ±n. AÅŸaÄŸÄ±daki baÄŸlamÄ± (context) kullanarak soruyu cevapla.

    KURALLAR:
    1. EÄŸer soru TÃ¼rkÃ§e ise cevabÄ± TÃœRKÃ‡E ver.
    2. EÄŸer soru Ä°ngilizce ise cevabÄ± Ä°NGÄ°LÄ°ZCE ver.
    3. BaÄŸlam (context) Ä°ngilizce olsa bile, sen her zaman SORUNUN DÄ°LÄ°NDE cevap ver.

    BaÄŸlam:
    {context}

    Soru: {question}

    Cevap:
    """
    prompt = ChatPromptTemplate.from_template(template)

    def format_docs(docs):
        return "\n\n".join(doc.page_content for doc in docs)

    rag_chain = (
            {"context": retriever | format_docs, "question": RunnablePassthrough()}
            | prompt
            | llm
            | StrOutputParser()
    )
    return rag_chain


# ---------------------------------------------------------
# 4. ARAYÃœZ MANTIÄI (SENÄ°N KODUNDA EKSÄ°K OLAN KISIM)
# ---------------------------------------------------------

# Sistemin hazÄ±r olup olmadÄ±ÄŸÄ±nÄ± kontrol et
try:
    retriever = get_retriever()
    chain = get_chain(retriever)
    st.success("Sistem HazÄ±r! âœ… Sohbet edebilirsiniz.")
except Exception as e:
    st.error(f"Bir hata oluÅŸtu: {e}")
    st.stop()

# Sohbet geÃ§miÅŸini hafÄ±zada tut
if "messages" not in st.session_state:
    st.session_state.messages = []

# Eski mesajlarÄ± ekrana bas
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# KullanÄ±cÄ±dan girdi al
if prompt := st.chat_input("Sorunuzu buraya yazÄ±n..."):
    # KullanÄ±cÄ± mesajÄ±nÄ± ekle
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    # Asistan cevabÄ±nÄ± Ã¼ret
    with st.chat_message("assistant"):
        with st.spinner("Cevap oluÅŸturuluyor..."):
            response = chain.invoke(prompt)
            st.markdown(response)

    # Asistan mesajÄ±nÄ± kaydet
    st.session_state.messages.append({"role": "assistant", "content": response})